# Домашнее задание 11

1. Инициализирую окружения для работы с YCloud
```bash
YC_TOKEN=$(yc iam create-token)
YC_CLOUD_ID=$(yc config get cloud-id)
YC_FOLDER_ID=$(yc config get folder-id)
```
2. Создал кластера Kubernetes и Container Registry
```bash
curl -L -o /usr/bin/jq.exe https://github.com/stedolan/jq/releases/latest/download/jq-win64.exe

  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
100 3442k  100 3442k    0     0   772k      0  0:00:04  0:00:04 --:--:-- 1201k

```
```bash
SA_ID=$(yc iam service-account get --name fed-semen-servise --format json | jq .id -r)
```
```bash
yc managed-kubernetes cluster create \
 --name k8s-demo --network-name default \
 --zone ru-central1-a  --subnet-name default-ru-central1-a \
 --public-ip \
 --service-account-id ${SA_ID} --node-service-account-id ${SA_ID} --async

id: catlegcnbnef52qheeg7
description: Create cluster
created_at: "2023-07-18T13:02:21.119343085Z"
created_by: ajejn3n4gpj9cup44v4k
modified_at: "2023-07-18T13:02:21.119343085Z"
metadata:
  '@type': type.googleapis.com/yandex.cloud.k8s.v1.CreateClusterMetadata
  cluster_id: catj9c7odp8i4dlitcko
```
3. загрузим kubeconfig для работы с кластером
```bash 
yc managed-kubernetes cluster get-credentials --external --name k8s-demo
```
4. Создадим группу узлов. 

Яндекс не дал создать три SSD по 96Гб из-за превышения лимита на общий размер сетевых ssd-дисков, уменьшил до 40Гб
```bash
$ yc managed-kubernetes node-group create  --name k8s-demo-ng  --cluster-name k8s-demo  --platform-id standard-v2  --cores 2  --memory 4  --core-fraction 50  --disk-type network-hdd --disk-size 40 --fixed-size 3  --location subnet-name=default-ru-central1-b,zone=ru-central1-b  --async
id: catui1lmjm0g1u86cpm1
description: Create node group
created_at: "2023-07-18T13:40:06.871774596Z"
created_by: ajejn3n4gpj9cup44v4k
modified_at: "2023-07-18T13:40:06.871774596Z"
metadata:
  '@type': type.googleapis.com/yandex.cloud.k8s.v1.CreateNodeGroupMetadata
  node_group_id: catu1c5upvmalo7r42eq

``` 
```bash 
$ yc container cluster list-node-groups catj9c7odp8i4dlitcko
+----------------------+-------------+----------------------+---------------------+---------+------+
|          ID          |    NAME     |  INSTANCE GROUP ID   |     CREATED AT      | STATUS  | SIZE |
+----------------------+-------------+----------------------+---------------------+---------+------+
| catu1c5upvmalo7r42eq | k8s-demo-ng | cl1gmvn041l1sovie8qa | 2023-07-18 13:40:06 | RUNNING |    3 |
+----------------------+-------------+----------------------+---------------------+---------+------+
``` 
```bash 
$ yc container cluster list-nodes catj9c7odp8i4dlitcko
+--------------------------------+---------------------------+--------------------------------+-------------+--------+
|         CLOUD INSTANCE         |      KUBERNETES NODE      |           RESOURCES            |    DISK     | STATUS |
+--------------------------------+---------------------------+--------------------------------+-------------+--------+
| epdl98scb9nsu0os8h1v           | cl1gmvn041l1sovie8qa-atos | 2 50% core(s), 4.0 GB of       | 40.0 GB hdd | READY  |
| RUNNING_ACTUAL                 |                           | memory                         |             |        |
| epdi4ivtdv6ievgfep9j           | cl1gmvn041l1sovie8qa-ibob | 2 50% core(s), 4.0 GB of       | 40.0 GB hdd | READY  |
| RUNNING_ACTUAL                 |                           | memory                         |             |        |
| epda23u50jpce0sk7ds0           | cl1gmvn041l1sovie8qa-oheh | 2 50% core(s), 4.0 GB of       | 40.0 GB hdd | READY  |
| RUNNING_ACTUAL                 |                           | memory                         |             |        |
+--------------------------------+---------------------------+--------------------------------+-------------+--------+
``` 
```bash  
$ kubectl get nodes -o wide
NAME                        STATUS   ROLES    AGE    VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
cl1gmvn041l1sovie8qa-atos   Ready    <none>   2m5s   v1.24.8   10.129.0.9    <none>        Ubuntu 20.04.5 LTS   5.4.0-139-generic   containerd://1.6.18
cl1gmvn041l1sovie8qa-ibob   Ready    <none>   118s   v1.24.8   10.129.0.30   <none>        Ubuntu 20.04.5 LTS   5.4.0-139-generic   containerd://1.6.18
cl1gmvn041l1sovie8qa-oheh   Ready    <none>   2m2s   v1.24.8   10.129.0.26   <none>        Ubuntu 20.04.5 LTS   5.4.0-139-generic   containerd://1.6.18
``` 
Ноды создались

5. Пробую развернуть кластер CitusDB/

Пробовал все образы от 7.3.0 до 10.1.1-pg12 и 12.0.0. Везде ошибка с таймаутом
```bash
kubectl apply -f secrets.yaml -f master.yaml -f workers.yaml
secret/citus-secrets created
persistentvolumeclaim/citus-master-pvc created
service/citus-master created
deployment.apps/citus-master created
service/citus-workers created
statefulset.apps/citus-worker created
```
В результате получаю
```bash
$ kubectl get pods
NAME                            READY   STATUS              RESTARTS   AGE
citus-master-86df8459d4-p4k6h   0/1     ImagePullBackOff   0          2m2s
citus-worker-0                  0/1     ImagePullBackOff   0          2m2s
```
```bash
$ kubectl describe pods citus-master-86df8459d4-p4k6h
Name:             citus-master-86df8459d4-p4k6h
Namespace:        default
Priority:         0
Service Account:  default
Node:             cl1gmvn041l1sovie8qa-atos/10.129.0.9
Start Time:       Tue, 18 Jul 2023 17:01:19 +0300
Labels:           app=citus-master
                  pod-template-hash=86df8459d4
Annotations:      <none>
Status:           Pending
IP:               10.112.132.6
IPs:
  IP:           10.112.132.6
Controlled By:  ReplicaSet/citus-master-86df8459d4
Containers:
  citus:
    Container ID:
    Image:          citusdata/citus:7.3.0
    Image ID:
    Port:           5432/TCP
    Host Port:      0/TCP
    State:          Waiting
      Reason:       ImagePullBackOff
    Ready:          False
    Restart Count:  0
    Liveness:       exec [./pg_healthcheck] delay=60s timeout=1s period=10s #success=1 #failure=3
    Environment:
      PGDATA:             /var/lib/postgresql/data/pgdata
      PGPASSWORD:         <set to the key 'password' in secret 'citus-secrets'>  Optional: false
      POSTGRES_PASSWORD:  <set to the key 'password' in secret 'citus-secrets'>  Optional: false
    Mounts:
      /var/lib/postgresql/data from storage (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-xv5df (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             False
  ContainersReady   False
  PodScheduled      True
Volumes:
  storage:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  citus-master-pvc
    ReadOnly:   false
  kube-api-access-xv5df:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason                  Age                  From                     Message
  ----     ------                  ----                 ----                     -------
  Normal   Scheduled               3m40s                default-scheduler        Successfully assigned default/citus-master-86df8459d4-p4k6h to cl1gmvn041l1sovie8qa-atos
  Normal   SuccessfulAttachVolume  3m27s                attachdetach-controller  AttachVolume.Attach succeeded for volume "pvc-960656e7-7790-439f-be51-18a3a5515cbb"
  Warning  Failed                  2m56s                kubelet                  Failed to pull image "citusdata/citus:7.3.0": rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/citusdata/citus:7.3.0": failed to resolve reference "docker.io/citusdata/citus:7.3.0": failed to do request: Head "https://registry-1.docker.io/v2/citusdata/citus/manifests/7.3.0": dial tcp 44.205.64.79:443: i/o timeout
  Warning  Failed                  2m13s                kubelet                  Failed to pull image "citusdata/citus:7.3.0": rpc error: code = DeadlineExceeded desc = failed to pull and unpack image "docker.io/citusdata/citus:7.3.0": failed to resolve reference "docker.io/citusdata/citus:7.3.0": failed to do request: Head "https://registry-1.docker.io/v2/citusdata/citus/manifests/7.3.0": dial tcp 44.205.64.79:443: i/o timeout
  Warning  Failed                  79s (x3 over 2m56s)  kubelet                  Error: ErrImagePull
  Warning  Failed                  79s                  kubelet                  Failed to pull image "citusdata/citus:7.3.0": rpc error: code = DeadlineExceeded desc = failed to pull and unpack image "docker.io/citusdata/citus:7.3.0": failed to resolve reference "docker.io/citusdata/citus:7.3.0": failed to do request: Head "https://registry-1.docker.io/v2/citusdata/citus/manifests/7.3.0": dial tcp 34.205.13.154:443: i/o timeout
  Normal   BackOff                 51s (x4 over 2m56s)  kubelet                  Back-off pulling image "citusdata/citus:7.3.0"
  Warning  Failed                  51s (x4 over 2m56s)  kubelet                  Error: ImagePullBackOff
  Normal   Pulling                 37s (x4 over 3m26s)  kubelet                  Pulling image "citusdata/citus:7.3.0"

```
Так в итоге я и не понял как решить проблему, запускал все с локальной машины. 

6. Развернул кластер CitusDB в minikube.

Запустился образ 9.3.0. Все образы с версией выше тоже по таймауту не запускались

```bash
$ kubectl get pods
NAME                            READY   STATUS    RESTARTS        AGE
NAME                            READY   STATUS    RESTARTS        AGE
citus-master-64cc88d554-t8qxc   1/1     Running   0               2m53s
citus-worker-0                  1/1     Running   3 (2m30s ago)   2m53s
citus-worker-1                  1/1     Running   1 (2m3s ago)    2m5s
citus-worker-2                  1/1     Running   1 (115s ago)    2m1s
```
7. Захожу в мастер-ноду
```bash
$ kubectl exec -it citus-master-64cc88d554-t8qxc  -- bash
```
8. Скачал файл, который грузил в предыдущем ДЗ
```bash
mkdir /home/1
chmod 777 /home/1
cd /home/1
apt-get update
apt-get install wget
wget https://storage.googleapis.com/postgres13/1000000SalesRecords.csv

1000000SalesRecords.csv                             100%[=================================================================================================================>] 119.01M   719KB/s    in 3m 2s   

2023-07-18 15:49:32 (671 KB/s) - ‘1000000SalesRecords.csv’ saved [124793263/124793263]
```
9. Захожу в postgres
```sql
psql -U postgres
postgres=# \l
                                 List of databases
   Name    |  Owner   | Encoding |  Collate   |   Ctype    |   Access privileges
-----------+----------+----------+------------+------------+-----------------------
 postgres  | postgres | UTF8     | en_US.utf8 | en_US.utf8 |
 template0 | postgres | UTF8     | en_US.utf8 | en_US.utf8 | =c/postgres          +
           |          |          |            |            | postgres=CTc/postgres
 template1 | postgres | UTF8     | en_US.utf8 | en_US.utf8 | =c/postgres          +
           |          |          |            |            | postgres=CTc/postgres
(3 rows)

postgres=# SELECT * FROM master_get_active_worker_nodes();
 node_name  | node_port 
------------+-----------
 172.17.0.7 |      5432
 172.17.0.8 |      5432
 172.17.0.9 |      5432
(3 rows)
```
10. Создал таблицу test
```sql
postgres=# CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
CREATE EXTENSION
postgres=# CREATE TABLE test (
postgres(#     id UUID PRIMARY KEY DEFAULT uuid_generate_v1(),
postgres(#     Region VARCHAR(50),
postgres(#     Country VARCHAR(50),
postgres(#     ItemType VARCHAR(50),
postgres(#     SalesChannel VARCHAR(20),
postgres(#     OrderPriority VARCHAR(10),
postgres(#     OrderDate VARCHAR(10),
postgres(#     OrderID int,
postgres(#     ShipDate VARCHAR(10),
postgres(#     UnitsSold int,
postgres(#     UnitPrice decimal(12,2),
postgres(#     UnitCost decimal(12,2),
postgres(#     TotalRevenue decimal(12,2),
postgres(#     TotalCost decimal(12,2),
postgres(#     TotalProfit decimal(12,2)
postgres(# );
CREATE TABLE
postgres=# \dt+
                    List of relations
 Schema | Name | Type  |  Owner   |  Size   | Description
--------+------+-------+----------+---------+-------------
 public | test | table | postgres | 0 bytes |
(1 row)
```
11. Применим распределение по уникальному ключу
```sql
postgres=# SELECT create_distributed_table('test', 'id');
 create_distributed_table 
--------------------------

(1 row)
```
12. Включил замер времени
```sql
\timing
```
13. Загружу данные в созданную таблицу
```sql
postgres=# copy test (Region,Country,ItemType,SalesChannel,OrderPriority,OrderDate,OrderID,ShipDate,UnitsSold,UnitPrice,UnitCost,TotalRevenue,TotalCost,TotalProfit) FROM '/home/1/1000000SalesRecords.csv' DELIMITER ',' CSV HEADER;
COPY 1000000
Time: 29062.615 ms (00:29.063)
```
В прошлом ДЗ данные в ситус загрузились за 16 секунд, тут все логично, возможности моего локального ноута не такие, как у ВМ

14. Захожу в каждую ноду и смотрю количество таблиц распределения
```bash
fedse@DESKTOP-6G991ME MINGW64 /d/PG2/pgGKE/pgGKE/citus_93_ip
$ kubectl exec -it citus-worker-0  -- bash
root@citus-worker-0:/# psql -U postgres
psql (12.2 (Debian 12.2-2.pgdg100+1))
Type "help" for help.
```
```bash
postgres=# \dt+
                        List of relations
 Schema |    Name     | Type  |  Owner   |  Size   | Description
--------+-------------+-------+----------+---------+-------------
 public | test_102008 | table | postgres | 5144 kB |
 public | test_102011 | table | postgres | 5152 kB |
 public | test_102014 | table | postgres | 5192 kB |
 public | test_102017 | table | postgres | 5200 kB |
 public | test_102020 | table | postgres | 5168 kB |
 public | test_102023 | table | postgres | 5232 kB |
 public | test_102026 | table | postgres | 5144 kB |
 public | test_102029 | table | postgres | 5136 kB |
 public | test_102032 | table | postgres | 5208 kB |
 public | test_102035 | table | postgres | 5224 kB |
 public | test_102038 | table | postgres | 5168 kB |
(11 rows)

$ kubectl exec -it citus-worker-1  -- bash
root@citus-worker-1:/# psql -U postgres
psql (12.2 (Debian 12.2-2.pgdg100+1))
Type "help" for help.

postgres=# \dt+
                        List of relations
 Schema |    Name     | Type  |  Owner   |  Size   | Description
--------+-------------+-------+----------+---------+-------------
 public | test_102009 | table | postgres | 5192 kB |
 public | test_102012 | table | postgres | 5264 kB |
 public | test_102015 | table | postgres | 5184 kB |
 public | test_102018 | table | postgres | 5136 kB |
 public | test_102021 | table | postgres | 5160 kB |
 public | test_102024 | table | postgres | 5160 kB |
 public | test_102027 | table | postgres | 5144 kB |
 public | test_102030 | table | postgres | 5168 kB |
 public | test_102033 | table | postgres | 5216 kB |
 public | test_102036 | table | postgres | 5224 kB |
 public | test_102039 | table | postgres | 5176 kB |
(11 rows)

$ kubectl exec -it citus-worker-2  -- bash
root@citus-worker-2:/# psql -U postgres
psql (12.2 (Debian 12.2-2.pgdg100+1))
Type "help" for help.

postgres=# \dt+
                        List of relations
 Schema |    Name     | Type  |  Owner   |  Size   | Description
--------+-------------+-------+----------+---------+-------------
 public | test_102010 | table | postgres | 5176 kB |
 public | test_102013 | table | postgres | 5168 kB |
 public | test_102016 | table | postgres | 5200 kB |
 public | test_102019 | table | postgres | 5200 kB |
 public | test_102022 | table | postgres | 5200 kB |
 public | test_102025 | table | postgres | 5128 kB |
 public | test_102028 | table | postgres | 5112 kB |
 public | test_102031 | table | postgres | 5184 kB |
 public | test_102034 | table | postgres | 5144 kB |
 public | test_102037 | table | postgres | 5144 kB |
(10 rows)
```
В двух 11 таблиц, в одной - 10

15. Делаю в координаторе запрос из прошлого ДЗ
```sql
postgres=# select distinct region from test;
              region
-----------------------------------
 Middle East and North Africa      
 Sub-Saharan Africa
 Central America and the Caribbean 
 Asia
 Australia and Oceania
 North America
 Europe
(7 rows)

Time: 17445.274 ms (00:17.445)
```
Там запрос выполнялся 98.696 ms
