# Проектная работа.
## Построение отказоустойчивого кластера PostgreSQL
1. Исходные данные

    1.1. Hostname: server1c. (IP 192.168.15.245) Ubuntu. ВМ, на которой крутится кластер 1С:Предприятие

    1.2. Hostname: pg-server. (IP 192.168.15.244) Ubuntu. ВМ, в которой крутился боевой сервер БД. PostgreSQL 13.12 on x86_64-pc-linux-gnu, compiled by gcc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0, 64-bit без первичной инициализации кластера

    1.3. Hostname: kappgserv. (IP 192.168.17.96) Ubuntu. Старый сервер. Использовался для тестовых баз. Полностью очистили под кластер. Подняли PostgreSQL 13.12 on x86_64-pc-linux-gnu, compiled by gcc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0, 64-bit без первичной инициализации кластера

    1.4. Hostname: zveryokpg. (IP 192.168.17.65) Новая ВМ. Подняли PostgreSQL 13.12 on x86_64-pc-linux-gnu, compiled by gcc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0, 64-bit

    1.5. Все машины находятся в одной сети и видят друг друга

2. Схема реализации

    2.1. Кластер ETCD будет развернут на всех машинах: server1c, pg-server, kappgserv, zveryokpg

    2.2. Кластер Patroni будет развернут на машинах БД: pg-server, kappgserv, zveryokpg

    2.3. HAproxy. В настройках информационной базы, в кластере 1С:Предприятие, необходимо указывать имя сервера СУБД. В нашем случае им может быть один из серверов БД. Так как указать все имена сразу мы не можем, а роли серверов (ведущий сервер и резервный сервер) могут поменяться в любой момент, следует создать точку подключения к PostgreSQL. В качестве точки подключения будет выступать HAProxy установленный на server1c. В задачe HAProxy будет входить слежение за ролями серверов PostgreSQL и, в случае их изменения, оперативное перенаправление запросов от 1С:Предприятие к СУБД на новый ведущий сервер. HAproxy будет установлен на server1c 

3. Настройка ETCD

Устанавливаю на всех нодах etcd
```
sudo apt install -y etcd

etcd --version
etcd Version: 3.2.26
Git SHA: Not provided (use ./build instead of go build)
Go Version: go1.13.8
Go OS/Arch: linux/amd64
```
Создаю файл настройки на server-1c
```
sudo cat > temp.cfg << EOF 
ETCD_NAME="server-1c"
ETCD_LISTEN_CLIENT_URLS="http://0.0.0.0:2379"
ETCD_ADVERTISE_CLIENT_URLS="http://192.168.15.245:2379"
ETCD_LISTEN_PEER_URLS="http://0.0.0.0:2380"
ETCD_INITIAL_ADVERTISE_PEER_URLS="http://192.168.15.245:2380"
ETCD_INITIAL_CLUSTER_TOKEN="PatroniCluster"
ETCD_INITIAL_CLUSTER="server-1c=http://192.168.15.245:2380"
ETCD_INITIAL_CLUSTER_STATE="new"
ETCD_DATA_DIR="/var/lib/etcd"
EOF
sudo cat temp.cfg | sudo tee -a /etc/default/etcd
```
Запускаю службу и проверяю кластер
```
sudo systemctl start etcd
etcdctl cluster-health
member b4cc1d654dd2ae33 is healthy: got healthy result from http://192.168.15.245:2379
cluster is healthy
```
Добавляю etcd в автозапуск
```
sudo systemctl enable etcd
Synchronizing state of etcd.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable etcd
```
Добавляю второй узел pg-server в ETCD
На server-1c выполнил оповещение кластера о появлении нового узла
```
etcdctl member add pg-server http://192.168.15.244:2380
Added member named pg-server with ID c123a1e246cfe877 to cluster

ETCD_NAME="pg-server"
ETCD_INITIAL_CLUSTER="server-1c=http://192.168.15.245:2380,pg-server=http://192.168.15.244:2380"
ETCD_INITIAL_CLUSTER_STATE="existing"
```
Создаю файл настройки на pg-server
```
sudo cat > temp.cfg << EOF 
ETCD_NAME="pg-server"
ETCD_LISTEN_CLIENT_URLS="http://0.0.0.0:2379"
ETCD_ADVERTISE_CLIENT_URLS="http://192.168.15.244:2379"
ETCD_LISTEN_PEER_URLS="http://0.0.0.0:2380"
ETCD_INITIAL_ADVERTISE_PEER_URLS="http://192.168.15.244:2380"
ETCD_INITIAL_CLUSTER_TOKEN="PatroniCluster"
ETCD_INITIAL_CLUSTER="server-1c=http://192.168.15.245:2380,pg-server=http://192.168.15.244:2380"
ETCD_INITIAL_CLUSTER_STATE="existing"
ETCD_DATA_DIR="/var/lib/etcd"
EOF
sudo cat temp.cfg | sudo tee -a /etc/default/etcd
```
```
sudo systemctl start etcd

etcdctl cluster-health
member b4cc1d654dd2ae33 is healthy: got healthy result from http://192.168.15.245:2379
member c123a1e246cfe877 is healthy: got healthy result from http://192.168.15.244:2379
cluster is healthy
```
Добавляю третий узел kappgserv в ETCD
На server-1c выполнил оповещение кластера о появлении нового узла
```
etcdctl member add kappgserv http://192.168.17.96:2380
Added member named kappgserv with ID f2aeb69aaf7ffcbf to cluster

ETCD_NAME="kappgserv"
ETCD_INITIAL_CLUSTER="server-1c=http://192.168.15.245:2380,pg-server=http://192.168.15.244:2380,kappgserv=http://192.168.17.96:2380"
ETCD_INITIAL_CLUSTER_STATE="existing"
```
Создаю файл настройки на kappgserv
```
sudo cat > temp.cfg << EOF 
ETCD_NAME="kappgserv"
ETCD_LISTEN_CLIENT_URLS="http://0.0.0.0:2379"
ETCD_ADVERTISE_CLIENT_URLS="http://192.168.17.96:2379"
ETCD_LISTEN_PEER_URLS="http://0.0.0.0:2380"
ETCD_INITIAL_ADVERTISE_PEER_URLS="http://192.168.17.96:2380"
ETCD_INITIAL_CLUSTER_TOKEN="PatroniCluster"
ETCD_INITIAL_CLUSTER="server-1c=http://192.168.15.245:2380,pg-server=http://192.168.15.244:2380,kappgserv=http://192.168.17.96:2380"
ETCD_INITIAL_CLUSTER_STATE="existing"
ETCD_DATA_DIR="/var/lib/etcd"
EOF
sudo cat temp.cfg | sudo tee -a /etc/default/etcd
```
```
sudo systemctl start etcd

etcdctl cluster-health
member b4cc1d654dd2ae33 is healthy: got healthy result from http://192.168.15.245:2379
member c123a1e246cfe877 is healthy: got healthy result from http://192.168.15.244:2379
member f2aeb69aaf7ffcbf is healthy: got healthy result from http://192.168.17.96:2379
cluster is healthy
```
Добавляю четвертый узел zveryokpg в ETCD
На server-1c выполнил оповещение кластера о появлении нового узла
```
etcdctl member add zveryokpg http://192.168.17.65:2380
Added member named zveryokpg with ID 5c70a9543bce2aca to cluster

ETCD_NAME="kappgserv"
ETCD_INITIAL_CLUSTER="server-1c=http://192.168.15.245:2380,pg-server=http://192.168.15.244:2380,kappgserv=http://192.168.17.96:2380,zveryokpg=http://192.168.17.65:2380"
ETCD_INITIAL_CLUSTER_STATE="existing"
```
Создаю файл настройки на zveryokpg
```
sudo cat > temp.cfg << EOF 
ETCD_NAME="zveryokpg"
ETCD_LISTEN_CLIENT_URLS="http://0.0.0.0:2379"
ETCD_ADVERTISE_CLIENT_URLS="http://192.168.17.96:2379"
ETCD_LISTEN_PEER_URLS="http://0.0.0.0:2380"
ETCD_INITIAL_ADVERTISE_PEER_URLS="http://192.168.17.96:2380"
ETCD_INITIAL_CLUSTER_TOKEN="PatroniCluster"
ETCD_INITIAL_CLUSTER="server-1c=http://192.168.15.245:2380,pg-server=http://192.168.15.244:2380,kappgserv=http://192.168.17.96:2380,zveryokpg=http://192.168.17.65:2380"
ETCD_INITIAL_CLUSTER_STATE="existing"
ETCD_DATA_DIR="/var/lib/etcd"
EOF
sudo cat temp.cfg | sudo tee -a /etc/default/etcd
```
```
sudo systemctl start etcd

etcdctl cluster-health
member b4cc1d654dd2ae33 is healthy: got healthy result from http://192.168.15.245:2379
member c123a1e246cfe877 is healthy: got healthy result from http://192.168.15.244:2379
member f2aeb69aaf7ffcbf is healthy: got healthy result from http://192.168.17.96:2379
member 5c70a9543bce2aca is healthy: got healthy result from http://192.168.17.65:2379
cluster is healthy
```
Добавили etcd в автозапуск на всех серверах
```
sudo systemctl enable etcd.service 
Created symlink from /etc/systemd/system/multi-user.target.wants/etcd.service to /usr/lib/systemd/system/etcd.service.
```
Завершение установки etcd

После успешного запуска etcd на всех серверах, перевели содержание файла /etc/default/etcd в окончательное состояние. Для этого изменилиследующие параметры в этом файле на всех серверах: 
```
ETCD_INITIAL_CLUSTER="server-1c=http://192.168.15.245:2380,pg-server=http://192.168.15.244:2380,kappgserv=http://192.168.17.96:2380,zveryokpg=http://192.168.17.65:2380"
ETCD_INITIAL_CLUSTER_STATE="existing"
```
Создали пользователя в etcd

Добавим авторизацию по логину и паролю при обращениях на клиентский интерфейс etcd.

Создаём пользователя "root"
Копировать в буфер обмена
```
etcdctl user add root
New password:
User root created
```
```
etcdctl user get root
User: root
Roles: root
```
Включили проверку логина и пароля
```
etcdctl auth enable
Authentication Enabled
```
Проверили, что изменения вступили в силу
```
etcdctl user get root
Insufficient credentials
```
```
etcdctl --username root user get root
Password:
User: root
Roles: root
```
4. Настройка Patroni

Установили Python
```
sudo apt-get install -y python3 python3-pip git mc

python3 --version
Python 3.8.10
```
Установили зависимости
```
sudo pip3 install psycopg2-binary 
```
Устанавили Patroni
```
sudo pip3 install patroni[etcd]

patroni --version
patroni 3.1.0
```
Настроили pg-server:

Создаём каталог настроек Patroni
```
sudo mkdir /etc/patroni
sudo chown postgres:postgres /etc/patroni
sudo chmod 700 /etc/patroni
```
Создали файл настроек для службы patroni
```
nano /etc/patroni/patroni.yml

name: pg-server
namespace: /db/
scope: postgres
restapi:
  listen: 192.168.15.244:8008
  connect_address: 192.168.15.244:8008
  authentication:
   username: patroni
   password: patroni
etcd:
 hosts: 192.168.15.244:2379
 #username: root
 #password: root
bootstrap:
 dcs:
  ttl: 30
  loop_wait: 10
  retry_timeout: 10
  maximum_lag_on_failover: 1048576
  master_start_timeout: 10
  postgresql:
   use_pg_rewind: true
   use_slots: true
   parameters:
    wal_level: replica
    hot_standby: "on"
    wal_keep_segments: 8
    max_wal_senders: 5
    max_replication_slots: 5
    checkpoint_timeout: 30
 #initdb:
 #- auth-host: md5
 #- auth-local: peer
 #- encoding: UTF8
 #- data-checksums
 #- locale: ru_RU.UTF-8
 pg_hba:
 - host replication replicator samenet md5
 - host replication all 127.0.0.1/32 md5
 - host replication all ::1/128 md5
 users:
  postgres:
   password: 12345
   options:
    - superuser
postgresql:
 listen: 0.0.0.0:5432
 connect_address: 192.168.15.244:5432
 config_dir: /var/lib/pgpro/1c-13/data
 bin_dir: /opt/pgpro/1c-13/bin
 data_dir: /var/lib/pgpro/1c-13/data
 pgpass: /tmp/pgpass
 authentication:
   superuser:
     username: postgres
     password: 12345
   replication:
     username: replicator
     password: 12345
   rewind:
     username: patroni_rewind
     password: 12345
 parameters:
    max_connections: 40
    shared_buffers: 128MB
    dynamic_shared_memory_type: posix
    max_wal_size: 1GB
    min_wal_size: 80MB
    logging_collector: on
    log_timezone:'Etc/UTC'
    datestyle: 'iso, mdy'
    timezone: 'Etc/UTC'
    lc_messages: 'ru_RU.UTF-8'
    lc_monetary: 'ru_RU.UTF-8'
    lc_numeric: 'ru_RU.UTF-8'
    lc_time: 'ru_RU.UTF-8'
    default_text_search_config: 'pg_catalog.russian'
    listen_addresses: '*'
    shared_buffers: 7680MB
    temp_buffers: 128MB
    work_mem: 24MB
    maintenance_work_mem: 2GB
    max_files_per_process: 10000
    max_parallel_workers_per_gather: 4
    max_parallel_maintenance_workers: 4
    commit_delay: 1000
    max_wal_size: 16GB
    min_wal_size: 2GB
    checkpoint_timeout: 15min
    effective_cache_size: 23040MB
    from_collapse_limit: 8
    join_collapse_limit: 8
    autovacuum_max_workers: 4
    vacuum_cost_limit: 400
    autovacuum_naptime: 20s
    autovacuum_vacuum_scale_factor: 0.01
    autovacuum_analyze_scale_factor: 0.005
    max_locks_per_transaction: 256
    escape_string_warning: off   
    standard_conforming_strings: off
    shared_preload_libraries: 'online_analyze, plantuner'
    online_analyze.threshold: 50
    online_analyze.scale_factor: 0.1
    online_analyze.enable: on
    online_analyze.verbose: off
    online_analyze.min_interval: 10000
    online_analyze.table_type: 'temporary'
    plantuner.fix_empty_table: on
tags:
  nofailover: false
  noloadbalance: false
  clonefrom: false
  nosync: false
```
Настройки parameters были предварительно взяты из /var/lib/pgpro/1c-13/data/postgresql.conf

Создали сервис для запуска демона Patroni
```
sudo cat > temp.cfg << EOF 
[Unit]
Description=High availability PostgreSQL Cluster
After=syslog.target network.target
[Service]
Type=simple
User=postgres
Group=postgres
ExecStart=/usr/local/bin/patroni /etc/patroni/patroni.yml
KillMode=process
TimeoutSec=30
Restart=no
[Install]
WantedBy=multi-user.target
EOF
sudo cat temp.cfg | sudo tee -a /etc/systemd/system/patroni.service
```
Обновили системные настройки
```
sudo systemctl daemon-reload
```
и запустили
```
sudo systemctl start patroni.service

? patroni.service - High availability PostgreSQL Cluster
     Loaded: loaded (/etc/systemd/system/patroni.service; disabled; vendor preset: enabled)
     Active: active (running) since Tue 2023-09-12 08:12:37 UTC; 1s ago
   Main PID: 5952 (patroni)
      Tasks: 5 (limit: 2219)
     Memory: 25.5M
        CPU: 794ms
     CGroup: /system.slice/patroni.service
             └─5952 /usr/bin/python3 /usr/local/bin/patroni /etc/patroni/patroni.yml

Sep 12 08:12:37 pg-server systemd[1]: Started High availability PostgreSQL Cluster.
Sep 12 08:12:38 pg-server patroni[5952]: 2023-09-12 08:12:38,122 INFO: Selected new etcd server http://192.168.15.244:23>Sep 12 08:12:38 pg-server patroni[5952]: 2023-09-12 08:12:38,129 INFO: No PostgreSQL configuration items changed, not>Sep 12 08:12:38 pg-server patroni[5952]: 2023-09-12 08:12:38,131 INFO: establishing a new patroni connection to the p>Sep 12 08:12:38 pg-server patroni[5952]: 2023-09-12 08:12:38,552 WARNING: Could not activate Linux watchdog device: C>Sep 12 08:12:38 pg-server patroni[5952]: 2023-09-12 08:12:38,557 INFO: acquired session lock as a leader
```
Проверил подключение к кластеру PostgreSQL под пользователем usr1cv8
```
psql -U usr1cv8 -d postgres
Пароль пользователя postgres: 
psql (13.12)
Введите "help", чтобы получить справку.

postgres=#
```
Добавил patroni.service в автозапуск
```
sudo systemctl enable patroni.service
Created symlink /etc/systemd/system/multi-user.target.wants/patroni.service → /etc/systemd/system/patroni.service.
```
Проверил структуру каталогов кластера PostgreSQL
```
sudo ls -l /var/lib/pgpro/1c-13/data
total 260
drwx------ 11 postgres postgres  4096 Jun 28 08:55 base
-rw-------  1 postgres postgres    44 Sep 12 00:00 current_logfiles
drwx------  2 postgres postgres  4096 Sep 12 08:27 global
drwx------  2 postgres postgres 61440 Sep 12 00:00 log
drwx------  2 postgres postgres  4096 Dec 26  2021 pg_commit_ts
drwx------  2 postgres postgres  4096 Dec 26  2021 pg_dynshmem
-rw-------  1 postgres postgres  4538 Dec 26  2021 pg_hba.conf
-rw-------  1 postgres postgres  1636 Dec 26  2021 pg_ident.conf
drwx------  4 postgres postgres  4096 Sep 12 08:21 pg_logical
drwx------  4 postgres postgres  4096 Dec 26  2021 pg_multixact
drwx------  2 postgres postgres  4096 Dec 26  2021 pg_notify
drwx------  2 postgres postgres  4096 Dec 26  2021 pg_replslot
drwx------  2 postgres postgres  4096 Dec 26  2021 pg_serial
drwx------  2 postgres postgres  4096 Dec 26  2021 pg_snapshots
drwx------  2 postgres postgres  4096 Sep  5 16:20 pg_stat
drwx------  2 postgres postgres  4096 Sep 12 08:27 pg_stat_tmp
drwx------  2 postgres postgres  4096 Sep 12 07:51 pg_subtrans
drwx------  2 postgres postgres  4096 Dec 26  2021 pg_tblspc
drwx------  2 postgres postgres  4096 Dec 26  2021 pg_twophase
-rw-------  1 postgres postgres     3 Dec 26  2021 PG_VERSION
drwx------  3 postgres postgres 65536 Sep 12 08:21 pg_wal
drwx------  2 postgres postgres  4096 Sep 11 07:13 pg_xact
-rw-------  1 postgres postgres    88 Dec 26  2021 postgresql.auto.conf
-rw-------  1 postgres postgres 29158 Jun 25 21:27 postgresql.conf
-rw-------  1 postgres postgres    63 Sep 11 06:58 postmaster.opts
-rw-------  1 postgres postgres    82 Sep 11 06:58 postmaster.pid
```
Проверил,слушает ли PostgreSQL слушать порт 5432
```
/var/lib/pgpro/1c-13/data# ss -ltn | grep 5432
LISTEN 0      242      192.168.15.244:5432      0.0.0.0:*
LISTEN 0      242        127.0.0.1:5432      0.0.0.0:*
```
Проверил результат выполненных настроек
```
sudo patronictl -c /etc/patroni/patroni.yml list
+ Cluster: postgres ---------+--------+---------+----+-----------+-----------------+
| Member    | Host           | Role   | State   | TL | Lag in MB | Pending restart |
+-----------+----------------+--------+---------+----+-----------+-----------------+
| pg-server | 192.168.15.244 | Leader | running |  3 |           | *               |
+-----------+----------------+--------+---------+----+-----------+-----------------+
```
Добавил новый узел Patroni. Устанавливаю все то же самое, только меняю yaml
```
nano /etc/patroni/patroni.yml
```
```
name: kappgserv
namespace: /db/
scope: postgres
restapi:
  listen: 192.168.17.96:8008
  connect_address: 192.168.17.96:8008
  authentication:
   username: patroni
   password: patroni
etcd:
 hosts: 192.168.17.96:2379
 #username: root
 #password: root
bootstrap:
 dcs:
  ttl: 30
  loop_wait: 10
  retry_timeout: 10
  maximum_lag_on_failover: 1048576
  master_start_timeout: 10
  postgresql:
   use_pg_rewind: true
   #use_slots: true
   parameters:
    wal_level: replica
    hot_standby: "on"
    wal_keep_segments: 8
    max_wal_senders: 5
    max_replication_slots: 5
    checkpoint_timeout: 30
 #initdb:
 #- auth-host: md5
 #- auth-local: peer
 #- encoding: UTF8
 #- data-checksums
 #- locale: ru_RU.UTF-8
 pg_hba:
 - host replication replicator samenet md5
 - host replication all 127.0.0.1/32 md5
 - host replication all ::1/128 md5
 users:
  postgres:
   password: 12345
   options:
    - superuser
postgresql:
 listen: 0.0.0.0:5432
 connect_address: 192.168.17.96:5432
 config_dir: /var/lib/pgpro/1c-13/data
 bin_dir: /opt/pgpro/1c-13/bin
 data_dir: /var/lib/pgpro/1c-13/data
 pgpass: /tmp/pgpass
 authentication:
   superuser:
     username: postgres
     password: 12345
   replication:
     username: replicator
     password: 12345
   rewind:
     username: patroni_rewind
     password: 12345
 parameters:
    max_connections: 40
    shared_buffers: 128MB
    dynamic_shared_memory_type: posix
    max_wal_size: 1GB
    min_wal_size: 80MB
    logging_collector: on
    log_timezone:'Etc/UTC'
    datestyle: 'iso, mdy'
    timezone: 'Etc/UTC'
    lc_messages: 'ru_RU.UTF-8'
    lc_monetary: 'ru_RU.UTF-8'
    lc_numeric: 'ru_RU.UTF-8'
    lc_time: 'ru_RU.UTF-8'
    default_text_search_config: 'pg_catalog.russian'
    listen_addresses: '*'
    shared_buffers: 7680MB
    temp_buffers: 128MB
    work_mem: 24MB
    maintenance_work_mem: 2GB
    max_files_per_process: 10000
    max_parallel_workers_per_gather: 4
    max_parallel_maintenance_workers: 4
    commit_delay: 1000
    max_wal_size: 16GB
    min_wal_size: 2GB
    checkpoint_timeout: 15min
    effective_cache_size: 23040MB
    from_collapse_limit: 8
    join_collapse_limit: 8
    autovacuum_max_workers: 4
    vacuum_cost_limit: 400
    autovacuum_naptime: 20s
    autovacuum_vacuum_scale_factor: 0.01
    autovacuum_analyze_scale_factor: 0.005
    max_locks_per_transaction: 256
    escape_string_warning: off   
    standard_conforming_strings: off
    shared_preload_libraries: 'online_analyze, plantuner'
    online_analyze.threshold: 50
    online_analyze.scale_factor: 0.1
    online_analyze.enable: on
    online_analyze.verbose: off
    online_analyze.min_interval: 10000
    online_analyze.table_type: 'temporary'
    plantuner.fix_empty_table: on
tags:
  nofailover: false
  noloadbalance: false
  clonefrom: false
  nosync: false
```
Добавил новый узел Patroni. Устанавливаю все то же самое, только меняю yaml
```
nano /etc/patroni/patroni.yml
```
```
name: zveryokpg
namespace: /db/
scope: postgres
restapi:
  listen: 192.168.17.65:8008
  connect_address: 192.168.17.65:8008
  authentication:
   username: patroni
   password: patroni
etcd:
 hosts: 192.168.17.65:2379
 #username: root
 #password: root
bootstrap:
 dcs:
  ttl: 30
  loop_wait: 10
  retry_timeout: 10
  maximum_lag_on_failover: 1048576
  master_start_timeout: 10
  postgresql:
   use_pg_rewind: true
   #use_slots: true
   parameters:
    wal_level: replica
    hot_standby: "on"
    wal_keep_segments: 8
    max_wal_senders: 5
    max_replication_slots: 5
    checkpoint_timeout: 30
 #initdb:
 #- auth-host: md5
 #- auth-local: peer
 #- encoding: UTF8
 #- data-checksums
 #- locale: ru_RU.UTF-8
 pg_hba:
 - host replication replicator samenet md5
 - host replication all 127.0.0.1/32 md5
 - host replication all ::1/128 md5
 users:
  postgres:
   password: 12345
   options:
    - superuser
postgresql:
 listen: 0.0.0.0:5432
 connect_address: 192.168.17.65:5432
 config_dir: /var/lib/pgpro/1c-13/data
 bin_dir: /opt/pgpro/1c-13/bin
 data_dir: /var/lib/pgpro/1c-13/data
 pgpass: /tmp/pgpass
 authentication:
   superuser:
     username: postgres
     password: 12345
   replication:
     username: replicator
     password: 12345
   rewind:
     username: patroni_rewind
     password: 12345
 parameters:
    max_connections: 40
    shared_buffers: 128MB
    dynamic_shared_memory_type: posix
    max_wal_size: 1GB
    min_wal_size: 80MB
    logging_collector: on
    log_timezone:'Etc/UTC'
    datestyle: 'iso, mdy'
    timezone: 'Etc/UTC'
    lc_messages: 'ru_RU.UTF-8'
    lc_monetary: 'ru_RU.UTF-8'
    lc_numeric: 'ru_RU.UTF-8'
    lc_time: 'ru_RU.UTF-8'
    default_text_search_config: 'pg_catalog.russian'
    listen_addresses: '*'
    shared_buffers: 7680MB
    temp_buffers: 128MB
    work_mem: 24MB
    maintenance_work_mem: 2GB
    max_files_per_process: 10000
    max_parallel_workers_per_gather: 4
    max_parallel_maintenance_workers: 4
    commit_delay: 1000
    max_wal_size: 16GB
    min_wal_size: 2GB
    checkpoint_timeout: 15min
    effective_cache_size: 23040MB
    from_collapse_limit: 8
    join_collapse_limit: 8
    autovacuum_max_workers: 4
    vacuum_cost_limit: 400
    autovacuum_naptime: 20s
    autovacuum_vacuum_scale_factor: 0.01
    autovacuum_analyze_scale_factor: 0.005
    max_locks_per_transaction: 256
    escape_string_warning: off   
    standard_conforming_strings: off
    shared_preload_libraries: 'online_analyze, plantuner'
    online_analyze.threshold: 50
    online_analyze.scale_factor: 0.1
    online_analyze.enable: on
    online_analyze.verbose: off
    online_analyze.min_interval: 10000
    online_analyze.table_type: 'temporary'
    plantuner.fix_empty_table: on
tags:
  nofailover: false
  noloadbalance: false
  clonefrom: false
  nosync: false
```
После инициализации проверяю
```
sudo patronictl -c /etc/patroni/patroni.yml list
+ Cluster: postgres ---------+---------+---------+----+-----------+-----------------+
| Member    | Host           | Role    | State   | TL | Lag in MB | Pending restart |
+-----------+----------------+---------+---------+----+-----------+-----------------+
| kappgserv | 192.168.17.96  | Replica | running |  7 |           | *               |
| pg-server | 192.168.15.244 | Leader  | running |  7 |           | *               |
| zveryokpg | 192.168.17.65  | Replica | running |    |           | *               |
+-----------+----------------+---------+---------+----+-----------+-----------------+
```
Проверяю подключение к PostgreSQL
```
psql -U postgres -h 192.168.15.244
Password for user postgres: 
psql (13.12)
Type "help" for help.

postgres=# \l
                                   List of databases
    Name     |  Owner   | Encoding |   Collate   |    Ctype    |   Access privileges
-------------+----------+----------+-------------+-------------+-----------------------
 esty        | postgres | UTF8     | ru_RU.UTF-8 | ru_RU.UTF-8 |
 izh_docmngr | postgres | UTF8     | ru_RU.UTF-8 | ru_RU.UTF-8 |
 postgres    | postgres | UTF8     | ru_RU.UTF-8 | ru_RU.UTF-8 |
 template0   | postgres | UTF8     | ru_RU.UTF-8 | ru_RU.UTF-8 | =c/postgres          +
             |          |          |             |             | postgres=CTc/postgres
 template1   | postgres | UTF8     | ru_RU.UTF-8 | ru_RU.UTF-8 | =c/postgres          +
             |          |          |             |             | postgres=CTc/postgres
(5 rows)

```
Ставлю службу в автозагрузку sudo systemctl enable patroni

5. Установка балансировщика "HAproxy" на сервер "server1c":
 
Устанавливаем "HAproxy", так же перемещаем настройки по умолчанию в сторону:
```
sudo apt install -y haproxy
mv /etc/haproxy/haproxy.cfg /etc/haproxy/haproxy.conf.def
```
Создаём конфигурационный файл:
```
nano /etc/haproxy/haproxy.cfg
global
 maxconn 100
defaults
 log global
 mode tcp
 retries 2
 timeout client 30m
 timeout connect 4s
 timeout server 30m
 timeout check 5s
listen stats
 mode http
 bind *:7000
 stats enable
 stats uri /
listen postgres
 bind *:5432
 option httpchk
 http-check expect status 200
 default-server inter 3s fastinter 1s fall 2 rise 2 on-marked-down shutdown-sessions
 server pg-server 192.168.15.244:5432 maxconn 100 check port 8008
 server kappgserv 192.168.17.96:5432 maxconn 100 check port 8008
 server zveryokpg 192.168.17.65:5432 maxconn 100 check port 8008
```
Запускаем службу "HAproxy", и открываем порты:
```
systemctl start haproxy.service
```
```
systemctl status haproxy.service

haproxy.service - HAProxy Load Balancer
     Loaded: loaded (/lib/systemd/system/haproxy.service; enabled; vendor preset: enabled)
     Active: active (running) since Tue 2023-09-12 10:27:22 UTC; 4min 8s ago
       Docs: man:haproxy(1)
             file:/usr/share/doc/haproxy/configuration.txt.gz
    Process: 5859 ExecStartPre=/usr/sbin/haproxy -Ws -f $CONFIG -c -q $EXTRAOPTS (code=exited, status=0/SUCCESS)        
   Main PID: 5863 (haproxy)
      Tasks: 3 (limit: 2219)
     Memory: 69.4M
        CPU: 101ms
     CGroup: /system.slice/haproxy.service
             ├─5863 /usr/sbin/haproxy -Ws -f /etc/haproxy/haproxy.cfg -p /run/haproxy.pid -S /run/haproxy-master.sock   
             └─5865 /usr/sbin/haproxy -Ws -f /etc/haproxy/haproxy.cfg -p /run/haproxy.pid -S /run/haproxy-master.sock   

Sep 12 10:27:22 server-1c systemd[1]: Starting HAProxy Load Balancer...
Sep 12 10:27:22 server-1c haproxy[5863]: [NOTICE]   (5863) : New worker #1 (5865) forked
Sep 12 10:27:22 server-1c systemd[1]: Started HAProxy Load Balancer.
```
Ставим в автозагрузку
```
systemctl enable haproxy.service
Synchronizing state of haproxy.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable haproxy
```
Выполняю на мастере перезагрузку
```
shutdown -r

+ Cluster: postgres ---------+---------+---------+----+-----------+-----------------+
| Member    | Host           | Role    | State   | TL | Lag in MB | Pending restart |
+-----------+----------------+---------+---------+----+-----------+-----------------+
| kappgserv | 192.168.17.96  | Leader  | running |  7 |           | *               |
| pg-server | 192.168.15.244 | Replica | running |  7 |           | *               |
| zveryokpg | 192.168.17.65  | Replica | running |    |           | *               |
+-----------+----------------+---------+---------+----+-----------+-----------------+
```
Пользователи видят следующее 

[here](file:///C:/Users/fedse/Downloads/238b016746478a9d35ca123a130ef923.png)


Но после перезахода в 1с все работает

Для реализации использовал следующие источники:
1. https://otus.ru/learning/231837/#/ "Кластер Patroni on-premise 1" и "Кластер Patroni on-premise 2"
2. https://its.1c.ru/db/metod8dev/content/5971/hdoc
